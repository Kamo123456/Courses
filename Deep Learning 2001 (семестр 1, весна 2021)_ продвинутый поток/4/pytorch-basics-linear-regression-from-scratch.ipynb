{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"pytorch-basics-linear-regression-from-scratch.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"4rxNlRu1tuQ8"},"source":["# PyTorch basics - Linear Regression from scratch\n","\n","<!-- <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ECHX1s0Kk-o?controls=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> -->\n","\n","Tutorial inspired from [FastAI development notebooks](https://github.com/fastai/fastai_v1/tree/master/dev_nb)\n","\n","## Machine Learning\n","\n","<img src=\"https://i.imgur.com/oJEQe7k.png\" width=\"500\">\n","\n","\n","## Tensors & Gradients"]},{"cell_type":"code","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"id":"KJu2Zj0ztuRF"},"source":["# Import Numpy & PyTorch\n","import numpy as np\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"34f006aa7eb4bbc683c39b7059021da900180908","id":"Ru5pAhJZtuRG"},"source":["A tensor is a number, vector, matrix or any n-dimensional array."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"e22be3f71825128f990e78959fa00d1331d344e4","id":"g65Vhq_xtuRG"},"source":["# Create tensors.\n","x = torch.tensor(3.)\n","w = torch.tensor(4., requires_grad=True)\n","b = torch.tensor(5., requires_grad=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"3cb90767ff9bc2c12b72548b1a430984241d4910","id":"lh8yRTr3tuRH"},"source":["# Print tensors\n","print(x)\n","print(w)\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"66a939ee0ec472705acd3f23654bc3ccea1cc8b4","id":"ccvIaC1MtuRH"},"source":["We can combine tensors with the usual arithmetic operations."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"0bd8fdeb252742e3449b7a2f08bcb188645dc9cf","id":"klyhXCBVtuRI"},"source":["# Arithmetic operations\n","y = w * x + b\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"64e0f175c65c3e875c671c40e4a9bf495e30b772","id":"YUEg6NJetuRI"},"source":["What makes PyTorch special, is that we can automatically compute the derivative of `y` w.r.t. the tensors that have `requires_grad` set to `True` i.e. `w` and `b`."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"6c98996f00294f99eb11989b5a9ecdbda31864e1","id":"xW0Psiv-tuRI"},"source":["# Compute gradients\n","y.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"47a62ffb26a76329e511f9f063c4c26cc6a7dc21","id":"IojYTdhwtuRJ"},"source":["# Display gradients\n","print('dy/dw:', w.grad)\n","print('dy/db:', b.grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"0b65b6bb4d15127b1d51f09abf616cfd29fa48b4","id":"4D4wNGAktuRJ"},"source":["## Problem Statement"]},{"cell_type":"markdown","metadata":{"_uuid":"c1beecda01bc332596edd193cade30006e3f6cbf","id":"hD-fzl4ItuRJ"},"source":["We'll create a model that predicts crop yeilds for apples and oranges (*target variables*) by looking at the average temperature, rainfall and humidity (*input variables or features*) in a region. Here's the training data:\n","\n","<img src=\"https://i.imgur.com/lBguUV9.png\" width=\"500\" />\n","\n","In a **linear regression** model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n","\n","```\n","yeild_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\n","yeild_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n","```\n","\n","Visually, it means that the yield of apples is a linear or planar function of the temperature, rainfall & humidity.\n","\n","<img src=\"https://i.imgur.com/mtkR2lB.png\" width=\"540\" >\n","\n","\n","**Our objective**: Find a suitable set of *weights* and *biases* using the training data, to make accurate predictions."]},{"cell_type":"markdown","metadata":{"_uuid":"c24b8195c0e9c6e8e13e169d264484f1f9b3b1ae","id":"7_JbicG0tuRK"},"source":["## Training Data\n","The training data can be represented using 2 matrices (inputs and targets), each with one row per observation and one column per variable."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"dfda99005fc6daf3a49ae1cdd427ccac0aa446b1","id":"XY-QF1dQtuRK"},"source":["# Input (temp, rainfall, humidity)\n","inputs = np.array([[73, 67, 43], \n","                   [91, 88, 64], \n","                   [87, 134, 58], \n","                   [102, 43, 37], \n","                   [69, 96, 70]], dtype='float32')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"bf56faf74f7e29c9ed7523308718a9ab1acc0667","id":"167b3Jt9tuRK"},"source":["# Targets (apples, oranges)\n","targets = np.array([[56, 70], \n","                    [81, 101], \n","                    [119, 133], \n","                    [22, 37], \n","                    [103, 119]], dtype='float32')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"70d48f83ae4fce7aba7dd78fd58dddc77c598bfd","id":"607wWoEktuRL"},"source":["Before we build a model, we need to convert inputs and targets to PyTorch tensors."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"931c1bad8788e607fa100d4338e1b1fe120e2339","id":"sgCmkDYTtuRL"},"source":["# Convert inputs and targets to tensors\n","inputs = torch.from_numpy(inputs)\n","targets = torch.from_numpy(targets)\n","print(inputs)\n","print(targets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"652647cd90bd0784ec4dc53472410f7358ee18c9","id":"Ej0yIhdetuRL"},"source":["## Linear Regression Model (from scratch)\n","\n","The *weights* and *biases* can also be represented as matrices, initialized with random values. The first row of `w` and the first element of `b` are use to predict the first target variable i.e. yield for apples, and similarly the second for oranges."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"6f788ae559355b3f01667be1554a5d2bdcade8db","id":"GcT2Ac4otuRL"},"source":["# Weights and biases\n","w = torch.randn(2, 3, requires_grad=True)\n","b = torch.randn(2, requires_grad=True)\n","print(w)\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"3579a065997cae41f7f504916b6bc07878ac768c","id":"pZkyF3SDtuRM"},"source":["The *model* is simply a function that performs a matrix multiplication of the input `x` and the weights `w` (transposed) and adds the bias `b` (replicated for each observation).\n","\n","$$\n","\\hspace{2.5cm} X \\hspace{1.1cm} \\times \\hspace{1.2cm} W^T \\hspace{1.2cm}  + \\hspace{1cm} b \\hspace{2cm}\n","$$\n","\n","$$\n","\\left[ \\begin{array}{cc}\n","73 & 67 & 43 \\\\\n","91 & 88 & 64 \\\\\n","\\vdots & \\vdots & \\vdots \\\\\n","69 & 96 & 70\n","\\end{array} \\right]\n","%\n","\\times\n","%\n","\\left[ \\begin{array}{cc}\n","w_{11} & w_{21} \\\\\n","w_{12} & w_{22} \\\\\n","w_{13} & w_{23}\n","\\end{array} \\right]\n","%\n","+\n","%\n","\\left[ \\begin{array}{cc}\n","b_{1} & b_{2} \\\\\n","b_{1} & b_{2} \\\\\n","\\vdots & \\vdots \\\\\n","b_{1} & b_{2} \\\\\n","\\end{array} \\right]\n","$$"]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"b1119f5ae9688a5f31dba438c7f78ca382deb7e3","id":"GeiY1QxAtuRM"},"source":["# Define the model\n","def model(x):\n","    return x @ w.t() + b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"8e0a4644cb1c4ed68a3bcf67a8a156341ac7c853","id":"Jz4UanFOtuRM"},"source":["The matrix obtained by passing the input data to the model is a set of predictions for the target variables."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"b042a3cf8f16f4c4380cccbac9d0892719c24190","id":"M3Xd8VMVtuRN"},"source":["# Generate predictions\n","preds = model(inputs)\n","print(preds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"5551ef933de7902c8b5a38ae3d8e4795cb244f38","id":"EWtWyYWRtuRN"},"source":["# Compare with targets\n","print(targets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"2c4a9cf2b3c9152f2f832176bce9a87381e2419c","id":"TdijJyT5tuRN"},"source":["Because we've started with random weights and biases, the model does not a very good job of predicting the target varaibles."]},{"cell_type":"markdown","metadata":{"_uuid":"edaae7266f5d47c5e970e1438a812f10d8d35fb4","id":"Lt-ZPucztuRN"},"source":["## Loss Function\n","\n","We can compare the predictions with the actual targets, using the following method: \n","* Calculate the difference between the two matrices (`preds` and `targets`).\n","* Square all elements of the difference matrix to remove negative values.\n","* Calculate the average of the elements in the resulting matrix.\n","\n","The result is a single number, known as the **mean squared error** (MSE)."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"dbf5bca8cbf2a3831089b454c70469e3748e9682","id":"mc7N1-njtuRO"},"source":["# MSE loss\n","def mse(t1, t2):\n","    diff = t1 - t2\n","    return torch.sum(diff * diff) / diff.numel()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"90da6779aad81608c40cdca77c3c04b68a815c11","id":"zzyAFLkWtuRO"},"source":["# Compute loss\n","loss = mse(preds, targets)\n","print(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"3ab3acadf389f30430b55c26c7979dcffaa974a5","id":"qTWc-gHwtuRO"},"source":["The resulting number is called the **loss**, because it indicates how bad the model is at predicting the target variables. Lower the loss, better the model. "]},{"cell_type":"markdown","metadata":{"_uuid":"c61acf9c3cff205d769fc52ed3b1b76f5ae66233","id":"eTqEtcETtuRO"},"source":["## Compute Gradients\n","\n","With PyTorch, we can automatically compute the gradient or derivative of the `loss` w.r.t. to the weights and biases, because they have `requires_grad` set to `True`."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"ef66710c6ef1944567c4dc033e1ca316f35490ab","id":"yfaPnW3otuRO"},"source":["# Compute gradients\n","loss.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"6504cddcfb4bfb0817bf03ef460f08f3145a9091","id":"PsUyw5WptuRP"},"source":["The gradients are stored in the `.grad` property of the respective tensors."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"5943d1cef604a178c95f5e8d255519d42d9f9982","id":"LwANOrHttuRP"},"source":["# Gradients for weights\n","print(w)\n","print(w.grad)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"47278e318b156c6a5812e0842dbc4164c8362562","id":"_tyjkmVetuRP"},"source":["# Gradients for bias\n","print(b)\n","print(b.grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"466dc3a2cc2d4bd2c10ae4cf59cf4627b5cc9c75","id":"GYBVwp0-tuRP"},"source":["A key insight from calculus is that the gradient indicates the rate of change of the loss, or the slope of the loss function w.r.t. the weights and biases. \n","\n","* If a gradient element is **postive**, \n","    * **increasing** the element's value slightly will **increase** the loss.\n","    * **decreasing** the element's value slightly will **decrease** the loss.\n","\n","<img src=\"https://i.imgur.com/2H4INoV.png\" width=\"400\" />\n","\n","\n","\n","* If a gradient element is **negative**,\n","    * **increasing** the element's value slightly will **decrease** the loss.\n","    * **decreasing** the element's value slightly will **increase** the loss.\n","    \n","<img src=\"https://i.imgur.com/h7E2uAv.png\" width=\"400\" />    \n","\n","The increase or decrease is proportional to the value of the gradient."]},{"cell_type":"markdown","metadata":{"_uuid":"35ed968bfc135bd44eeb100ae401d0628fbc5c63","id":"nN4gx1cQtuRQ"},"source":["Finally, we'll reset the gradients to zero before moving forward, because PyTorch accumulates gradients."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"5f02dc376c21857d4e545d98413952c5ac73039b","id":"x2QtsnlOtuRR"},"source":["w.grad.zero_()\n","b.grad.zero_()\n","print(w.grad)\n","print(b.grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"5501c66c9729c4954e9b798a0634a9d84487e639","id":"6raSuHuttuRR"},"source":["## Adjust weights and biases using gradient descent\n","\n","We'll reduce the loss and improve our model using the gradient descent algorithm, which has the following steps:\n","\n","1. Generate predictions\n","2. Calculate the loss\n","3. Compute gradients w.r.t the weights and biases\n","4. Adjust the weights by subtracting a small quantity proportional to the gradient\n","5. Reset the gradients to zero"]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"ef0d2bd2d9c5acb60992e238439ee00c2223319f","id":"s1zXsLeJtuRR"},"source":["# Generate predictions\n","preds = model(inputs)\n","print(preds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"302ee8226da4ee5d0dad137c638573a79f8abded","id":"_Xm4sqrxtuRR"},"source":["# Calculate the loss\n","loss = mse(preds, targets)\n","print(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"01c596aecf87e4670033ddd4ed36e26b97e2f9ab","id":"-A82PIMVtuRS"},"source":["# Compute gradients\n","loss.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"ec1e2bdc8f91523e556fad55ee8c01eb5431ae24","id":"dpV2zyZxtuRS"},"source":["# Adjust weights & reset gradients\n","with torch.no_grad():\n","    w -= w.grad * 1e-5\n","    b -= b.grad * 1e-5\n","    w.grad.zero_()\n","    b.grad.zero_()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"1d61b6f61f49b19099d29d1be8ec5ae4967bbd51","id":"Yg7FosRNtuRS"},"source":["print(w)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"6af10c29db7cb0d6e869b2c30966a34a48a011e2","id":"A73AF5b_tuRS"},"source":["With the new weights and biases, the model should have a lower loss."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"c542b5fe75d82454f34cac13cdcff8b48dd1945c","id":"7Q4LTK6utuRS"},"source":["# Calculate loss\n","preds = model(inputs)\n","loss = mse(preds, targets)\n","print(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"5201901695f3ea13d7fdd5d985da7e0761c541d0","id":"hw8luKkCtuRT"},"source":["## Train for multiple epochs\n","\n","To reduce the loss further, we repeat the process of adjusting the weights and biases using the gradients multiple times. Each iteration is called an epoch."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"9f5f0ffeee666b30c5828636359f0be6addbef7c","id":"xvkiQ0RotuRT"},"source":["# Train for 100 epochs\n","for i in range(100):\n","    preds = model(inputs)\n","    loss = mse(preds, targets)\n","    loss.backward()\n","    with torch.no_grad():\n","        w -= w.grad * 1e-5\n","        b -= b.grad * 1e-5\n","        w.grad.zero_()\n","        b.grad.zero_()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"c4820ca48b78f4dc242d80a9ec3ec6aca1aef671","id":"CjHkvF3HtuRT"},"source":["# Calculate loss\n","preds = model(inputs)\n","loss = mse(preds, targets)\n","print(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"bbcd65fa7094cec187565e54c2107e683bea787b","id":"OKv5ipx8tuRU"},"source":["# Print predictions\n","preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"addec2c4eca8edfcae5544ea2cc717182c21d90f","id":"73MKzF1VtuRU"},"source":["# Print targets\n","targets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"ecc6e79cdfb6a8ca882895ccc895b61b960b0a04","id":"7G1JfGLJtuRU"},"source":["## Linear Regression Model using PyTorch built-ins\n","\n","Let's re-implement the same model using some built-in functions and classes from PyTorch."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"ce66cf0d09a3f38bf2f00ea40418c56d98f1f814","id":"FEuNxNJ6tuRU"},"source":["# Imports\n","import torch.nn as nn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"74bb18bd01ac809079eeb8d05695206e8ba02069","id":"uZdPv07qtuRU"},"source":["# Input (temp, rainfall, humidity)\n","inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70], [73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70], [73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70]], dtype='float32')\n","# Targets (apples, oranges)\n","targets = np.array([[56, 70], [81, 101], [119, 133], [22, 37], [103, 119], \n","                    [56, 70], [81, 101], [119, 133], [22, 37], [103, 119], \n","                    [56, 70], [81, 101], [119, 133], [22, 37], [103, 119]], dtype='float32')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"d94b355f55250e9c7dcff668920f02d7c5c04925","id":"RiNFfmDmtuRU"},"source":["inputs = torch.from_numpy(inputs)\n","targets = torch.from_numpy(targets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"a0665466eb5401f40a816b323a34450b2c052c41","id":"UIrhXZ1rtuRV"},"source":["### Dataset and DataLoader\n","\n","We'll create a `TensorDataset`, which allows access to rows from `inputs` and `targets` as tuples. We'll also create a DataLoader, to split the data into batches while training. It also provides other utilities like shuffling and sampling."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"206f5fd0473386476b23477bf38d2c327b6376c9","id":"WRIvTLYPtuRV"},"source":["# Import tensor dataset & data loader\n","from torch.utils.data import TensorDataset, DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"c47a4f2f86fda3918094e01cf7ab0698bbb5acc7","id":"RLjdot-FtuRV"},"source":["# Define dataset\n","train_ds = TensorDataset(inputs, targets)\n","train_ds[0:3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"0a2f69126319d738b82ae67d5d404ecd6161bfac","id":"Ge2NIhfotuRV"},"source":["# Define data loader\n","batch_size = 5\n","train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n","next(iter(train_dl))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"276a262e1b9e3a048bcd32989013f9c501c59037","id":"1zykU1wMtuRV"},"source":["### nn.Linear\n","Instead of initializing the weights & biases manually, we can define the model using `nn.Linear`."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"59da3506559a0640d80d18f77b02726a1757be2f","id":"bQSS4ux_tuRW"},"source":["# Define model\n","model = nn.Linear(3, 2)\n","print(model.weight)\n","print(model.bias)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"b3a4a8c499a4680f2533329712de034671dd1cdd","id":"jotEVzPutuRW"},"source":["### Optimizer\n","Instead of manually manipulating the weights & biases using gradients, we can use the optimizer `optim.SGD`."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"1848398bd1ced8c25a7bb55612cf32a774500280","id":"rDYagZ_PtuRW"},"source":["# Define optimizer\n","opt = torch.optim.SGD(model.parameters(), lr=1e-5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"28cbe62be55010bd11b31d819cff38da5a772b18","id":"Drw0aNRvtuRW"},"source":["### Loss Function\n","Instead of defining a loss function manually, we can use the built-in loss function `mse_loss`."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"69d7f4e8e27ccd077f711da27f8bede8aa711893","id":"PwJ0xeLntuRX"},"source":["# Import nn.functional\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"a02ff888ed4be720fd9ca376022d8fdcf2559683","id":"jHK-03MatuRX"},"source":["# Define loss function\n","loss_fn = F.mse_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"a540adf76725ea9968025f6c029fdd251bdada6c","id":"x1moUj3ptuRX"},"source":["loss = loss_fn(model(inputs), targets)\n","print(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"e833614a69ff18c554a3d89f643ae2f11e0260f6","id":"MjO5hXP0tuRX"},"source":["### Train the model\n","\n","We are ready to train the model now. We can define a utility function `fit` which trains the model for a given number of epochs."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"128bc7260221f5338edf8b503c75f0c7d1cce7e8","id":"sWddvS_MtuRX"},"source":["# Define a utility function to train the model\n","def fit(num_epochs, model, loss_fn, opt):\n","    for epoch in range(num_epochs):\n","        for xb,yb in train_dl:\n","            # Generate predictions\n","            pred = model(xb)\n","            loss = loss_fn(pred, yb)\n","            # Perform gradient descent\n","            loss.backward()\n","            opt.step()\n","            opt.zero_grad()\n","    print('Training loss: ', loss_fn(model(inputs), targets))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"ae8ca4686cf6a68f6c9ca93bf3d227abe96c2201","id":"-zxaCG7ftuRY"},"source":["# Train the model for 100 epochs\n","fit(100, model, loss_fn, opt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"32588a47d0478772a1f08fa55874a322630bd0b6","id":"NiHl02ThtuRY"},"source":["# Generate predictions\n","preds = model(inputs)\n","preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"12d757c0f37c2e3af65cf9d4b59878cc10c65acf","id":"Q-UsBocntuRY"},"source":["# Compare with targets\n","targets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"e182289ebf21d8296f11f13264c4732c100da14f","id":"nD09DbuYtuRY"},"source":["# Bonus: Feedfoward Neural Network\n","\n","![ffnn](https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Multi-Layer_Neural_Network-Vector-Blank.svg/400px-Multi-Layer_Neural_Network-Vector-Blank.svg.png)\n","\n","Conceptually, you think of feedforward neural networks as two or more linear regression models stacked on top of one another with a non-linear activation function applied between them.\n","\n","<img src=\"https://cdn-images-1.medium.com/max/1600/1*XxxiA0jJvPrHEJHD4z893g.png\" width=\"640\">\n","\n","To use a feedforward neural network instead of linear regression, we can extend the `nn.Module` class from PyTorch."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"c405e5075d6c4adb26ead75c17be90eaeb43f2d5","id":"tL0AzSN9tuRZ"},"source":["class SimpleNet(nn.Module):\n","    # Initialize the layers\n","    def __init__(self):\n","        super().__init__()\n","        self.linear1 = nn.Linear(3, 3)\n","        self.act1 = nn.ReLU() # Activation function\n","        self.linear2 = nn.Linear(3, 2)\n","    \n","    # Perform the computation\n","    def forward(self, x):\n","        x = self.linear1(x)\n","        x = self.act1(x)\n","        x = self.linear2(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"2448d9832722f4f2813f8bd80b91daefd901dc2e","id":"RmlrgpAptuRZ"},"source":["Now we can define the model, optimizer and loss function exactly as before."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"a51ca222c2ea037c3caccaeab98ccdbcc30800cf","id":"TOsCsN25tuRZ"},"source":["model = SimpleNet()\n","opt = torch.optim.SGD(model.parameters(), 1e-5)\n","loss_fn = F.mse_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"21000c9739ea39a173a256f87339bfc112c1a9b0","id":"lIe_HFh0tuRZ"},"source":["Finally, we can apply gradient descent to train the model using the same `fit` function defined earlier for linear regression.\n","\n","<img src=\"https://i.imgur.com/g7Rl0r8.png\" width=\"500\">"]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"e94de6868c76803a998c1c1934ed229c826f3b8c","id":"Pfyz5RQLtuRZ"},"source":["fit(100, model, loss_fn, opt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"1132733442c7e7847ebd4b28d8c53c40ab6289cc","id":"H_HpiYHVtuRZ"},"source":[""],"execution_count":null,"outputs":[]}]}